In the realm of deep learning, stabilizing and accelerating model training are critical goals, achieved notably through various normalization techniques. Batch Normalization (BN) is favored for its efficiency in large-batch scenarios, Layer Normalization (LN) excels in recurrent neural networks and is also the current preferred normalization in various Transformers architectures. Instance Normalization (IN) is preferred in style transfer applications, and Group Normalization (GN) is advantageous for tasks with smaller batch sizes or where BN's dependence on batch size is limiting. Each technique offers unique benefits tailored to specific settings, yet the quest for more adaptable and efficient normalization methods continues.

Adaptive Group Normalization (AGN) emerges as a pioneering approach, refining the concept of GN (See figure \ref{fig:norm_methods}). AGN dynamically clusters channels based on their statistical properties, allowing for variable group sizes and a more tailored normalization process. Unlike traditional methods, AGN's groups are determined not by arbitrary divisions but by the inherent statistics of the channels, ensuring a more natural and effective normalization. This novel mechanism enables AGN to adaptively optimize group configurations for enhanced model convergence, filling a gap left by existing normalization techniques.

The primary objective of AGN is to markedly improve model convergence. By leveraging channel statistics for group formation, AGN introduces a level of adaptability previously unseen in normalization practices. This approach ensures that normalization is closely aligned with the data's statistical characteristics, optimizing the training process.

AGN's methodology involves performing clustering on the first minibatch of each normalization layer every \(x\) epochs. This clustering utilizes a 2D representation of each channel's mean and variance, grouping channels based on these statistical metrics. The resulting groups, which may vary in size, are then normalized individually. Among various clustering techniques evaluated, K-means coupled with an Isolation Forest algorithm for outlier removal has proven most effective in our experiments.

The contributions of this paper are multifaceted. Firstly, AGN introduces a flexible, data-driven approach to normalization, enhancing model performance and convergence. Secondly, by adapting group sizes according to data statistics, AGN provides a novel solution to the limitations of fixed group sizes in GN. Lastly, our comprehensive experiments across several deep learning tasks demonstrate AGN's superior performance over traditional normalization methods, showcasing its potential to significantly advance the field of neural network training.